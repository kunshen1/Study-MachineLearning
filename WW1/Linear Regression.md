# Linear Regression

**Reference Link**:
- https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md
- https://zhuanlan.zhihu.com/p/139445419


## 概述
线性回归是ML中监督学习的一种基本方法。回归的全称是：Regression towards the mean。 也就是说找到一条直线尽可能的在一堆散点的中间， 找到这个直线的过程就是回归。
![image](./Picture/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.webp)

## 目的
在一堆散点中看不出具体关系，通过找到一条直线来体现数据的整体趋势，让数据的关系更加清晰明了，从而可以预测未来。

## 什么是“线性”
线性回归找到的是直线，线性关系不仅仅只能存在 2 个变量（二维平面）。3 个变量时（三维空间），线性关系就是一个平面，4 个变量时（四维空间），线性关系就是一个体。以此类推…

## 什么是线性回归
找线的过程所找的线直的（即是线性的），那么这个找直线的过程就是“线性回归”。
线性回归(LR)可分为：简单一元线性回归和多元线性回归，也就是平时接触的一次线性方程和多次线性方程，二者的主要区别也就是未知项的个数。

### 一元线性回归
一元线性方程的公式应该是大家非常熟悉的：y=wx+b 

给你两个点，你就能确定其中的参数 w 和 b。之后，我们就能在直角坐标系中画出相应的一条直线来。有了这条线，随意扔给你一个 x ，你都能够通过这个函数式子求出 y 的值。像上面这种，通过求函数中的参数来找直线的过程就是线性回归。而当我们只用一个 x 来预测 y，就是简单一元线性回归，也就是在找一个直线来拟合数据。统计不像数学那么精确，统计会将理论与实际间的差别表示出来，也就是“误差”。因此，统计世界中的公式会有一个小尾巴 ɛ，用来代表误差，即 y = w<sub>0</sub> + w<sub>1</sub>x + ɛ 

### 多元线性回归

在实际预测中，为了增加准确性，就需要多添加些预测信息，在ML中也成为特征。增加特征的同时就需要增加参数。
![image](./Picture/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)


## 损失函数
损失函数是衡量回归模型误差的函数，也就是我们要的“直线”的评价标准。这个函数的值越小，说明直线越能拟合我们的数据。
$J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$
![image](./Picture/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.webp)
![](./Picture/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%96%B9%E7%A8%8B.png)
![](./Picture/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%96%B9%E7%A8%8B1.png)

### 最小二乘法
求解让损失函数（SSE）最小化的参数向量 $\omega$，这种通过最小化真实值和预测值之间的SSE来求解参数的方法就叫做最小二乘法。

